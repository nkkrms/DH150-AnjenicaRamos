# Assignment02: User Testing
##### Anjenica (Nikki) Ramos
##### DH 150: User Experience Design, Spring 2020 - Prof. Cho

#### Working Interest: World Health Organization COVID-19 Site 

### Purpose of UT 
The World Health Organization is a trusted international health agency which coordinates health efforts within countries of the United Nations, and, thus, currently serves as a leading source towards COVID-19 initiatives. Their [Coronavirus information website](coronavirus.com) features factual basics, advice and resources for the public, as well as regularly updated regulations and statistics of the ongoing pandemic. In this assignment, I will be conducting a usability test to evaluate the site's interface in regards to goals of effectivity, efficiency, and satisfaction towards its purposes as a public resource. Though doing so, I will be able to observe the site in use through a participant's behavior, which can then provide insight towards improvement for the site. 

I will specifically focus on areas of concern revealed by prior heuristic evaluation: (1) consistency and standards, (2) flexibility and efficiency of us, and (3) aesthetics and minimal design. At first glance, the WHO COVID-19 site does not appear to be problematic, though, exploration reveals various aspects of its design and structuring that hinders its purpose. Predominantly, the site has little standardization in terms of its information's organization, resulting in users ineffectively spending their time sifting through the text-heavy content to find the information they seek. This can lead to a bothersome ordeal and discourage use of the site overall. Through user testing, I aim to capture evidence of global problems from a user's perspective in over to support later design improvements and embetter overall user experience. 


### Methodology
To do so, I will conduct a pilot test in a portable minimalistic lab (at home) as a moderator with a participant (my brother), in order to test the setting, materials, and software involved in this process. I will use my personal laptop with internet connection to access the site in question, Apowersoft Online Screen Recorder to capture the testing session, and Google Forms to host the formal evaluation.

I, as the moderator, will introduce the project and usability testing procedure to the participant. After consenting to the session, I will lead the participant through background, pre-test, task, and post-test questions. During this, the participant will fill out the online form evaluation and talk aloud their thinking process. First, we will capture their first impressions of the site UI prior to any interactions. Then, they will complete four (revised from the original three) tasks designed to highlight the site's problem areas and capture natural troubleshooting by the user. This is meant to test the general functionality of the site. Next, the user will be able to share their opinons of their experience, in addition to the site's effectivity, efficiency, and satisfaction factors. Finally, we will collect demographic data. 


#### [Online Survey](https://forms.gle/FPwEAZ3RF4yn9uPQ6) 
#### [Pilot UT](https://drive.google.com/file/d/1qsqXdVKyvy6t00NvPUqJdxRhpciL_sK1/view?usp=sharing)

### Reflection
In terms of what the user testing has revealed, there are definite issues with the irregular, and therefore, unpredictable nature, of the WHO COVID-19 site's interface. The format of how information itself, as well as links/buttons, varied page to page, resulting in the participant to spend much of their time scanning and scrolling to find the information. To me, I considered this a matter of inconsistency, but, as by participant/brother expressed after the formal session/recording ended, he thought it was also a matter of being *too* consistent -- in that he repeatedly looked passed things because they were not obvious enough or "jumped out" at him to draw his attention. This shifted my perspective to make sure I focus on standardizing certain elements to the site, while creating clearer information hierarchies and eye-catching displays (or use of icons/symbols?) elsewhere. In relation, it was also made clear that the site was text-heavy (paragraphs > bulletpoints, articles > infographics), such that it was ironic that pieces of critical information was time-consuming to locate. 

This overall covers the success of the UT session, as the testing definitely reveals insights that I can utilize for eventual designs. In addition, I took the route of tailoring portions of the Google Form evaluation provided to add an additional task and expand upon the word-card portion of the post-test. I believe this was beneficial in pointing out additional site problems and enabled me to receive more feedback of user experience, especially with only one participant. Some aspects of the users behavior (ex. actually reading through everything instead of immediately resulting to CTRL-F to find something) and opinon (ex. the aformentioned over-consistency) were not expected, but definitely expanded my own thinking. 

In relation to conducting the user testing itself, I definitely realize the difference in my perspective as the moderator/tester, as someone who has scowered through the site much more in depth. This definitely attests to gaining empathy towards participants, as the Prof. pointed out within the lecture, because there can be points of frustration and confusion throughout their experience. 

The portions which did not go so well generally regard fascilitating the UT process itself, though I believe it is all room for improvement that comes with more practice and application. First off, I did find a typo or two typos in my form that I missed despite revising it twice. More significantly, though, pilot testing made me realize that I had designed each task of the testing portion independently of eachother and with starting at the home page in mind -- and so I instinctively led the participant to go back to the starting page, remembering immediately after that I am not supposed to intervene this way. In other times, it was taking *active* charge of my role I need to improve upon: I needed to keep in mind that it is up to me as the moderator to read aloud segments of questionnaire and manage the pacing of it, as opposed to letting the participant do so, even though them verbalizing their thinking process led them to do so. 

From this, I would like to become more comfortable and commending as a moderator in the future. I would also suggest revising the Google Form's Post-Test (part 1's Likert rating) questions to become interlaced with each scenario instead of at the end, as the participant also commended that it was harder to recall each experience to assess alltogether afterwards. 
